import torch
import torch import torch .nn as nn
import torch.nn.functional as F
import math 

class MultiHeadAttention(nn.Module):
  def __init__(self, d-model, num_heads):
    super.().__init__()
    self.d_model = d_model
    self.num_heads = num_heads
    self.head_dim =d = 
    
    

